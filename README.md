# "Анализ публикуемых новостей" с использованием Airflow

## Общая задача

Проект "Анализ публикуемых новостей" направлен на создание процесса ETL (Extract, Transform, Load) для формирования витрин данных, необходимых для анализа публикаций новостей. Основной целью является сбор, обработка и представление информации о новостях из различных источников в удобном для анализа формате.

## Подробное описание задачи

### Скрипты загрузки данных

- Разработаны скрипты загрузки данных в двух режимах:
  - Инициализирующий — загрузка полного слепка данных источника.
  - Инкрементальный — загрузка дельты данных за прошедшие сутки.

### Структура хранения данных

- Организована структура хранения данных, включающая:
  - Сырой слой данных.
  - Промежуточный слой.
  - Слой витрин.

### Результаты работы программного продукта

- Написан скрипт, формирующий витрину данных, включающую следующую информацию:
  - Суррогатный ключ категории.
  - Название категории.
  - Общее количество новостей из всех источников по данной категории за все время.
  - Количество новостей данной категории для каждого из источников за все время.
  - Общее количество новостей из всех источников по данной категории за последние сутки.
  - Количество новостей данной категории для каждого из источников за последние сутки.
  - Среднее количество публикаций по данной категории в сутки.
  - День, в который было сделано максимальное количество публикаций по данной новости.
  - Количество публикаций новостей данной категории по дням недели.

### Дополнение

- Производится приведение разнообразных названий и категорий к единому виду для обеспечения единообразия данных.

### Источники

- Используются следующие источники новостей:
  - [Lenta.ru](https://lenta.ru/rss/)
  - [Vedomosti](https://www.vedomosti.ru/rss/news)
  - [TASS](https://tass.ru/rss/v2.xml)

### Схема базы

![Схема базы](shema.png)


### ETL-процесс

1. **Извлечение данных (Extract):**
   - Данные из различных RSS-каналов, таких как [Vedomosti](https://www.vedomosti.ru/rss/news), [Lenta.ru](https://lenta.ru/rss/), [TASS](https://tass.ru/rss/v2.xml), извлекаются с использованием библиотеки feedparser.
   - Загруженные данные сохраняются в сырой слой базы данных PostgreSQL.

2. **Трансформация данных (Transform):**
   - Данные подвергаются процессу трансформации, включающему приведение к единому формату категорий, обработку HTML-кода для получения текста новостей, а также создание связанных таблиц для нормализации базы данных.
   - Используется библиотека BeautifulSoup для парсинга HTML-кода новостей.

3. **Загрузка данных (Load):**
   - Преобразованные данные загружаются в промежуточный слой базы данных PostgreSQL.

4. **Формирование витрины данных:**
   - Создается витрина данных, предоставляющая информацию о категориях, количестве новостей, среднем количестве публикаций в сутки, и др.
   - Витрина формируется с использованием SQL-запросов, агрегирующих данные по категориям, сайтам и дням публикаций.

5. **Оркестрация через Apache Airflow:**
   - Airflow используется для автоматизации выполнения ETL-процесса.
   - Задачи в DAG (Directed Acyclic Graph) определены для каждого этапа процесса, включая извлечение, трансформацию и загрузку данных.
   - Используется DAG (Directed Acyclic Graph) в Apache Airflow для выполнения SQL-запроса, формирующего представление для анализа данных. Оператор PostgresOperator в Airflow используется для выполнения SQL-запроса

### Инструменты и технологии

- Используется язык программирования Python для написания скриптов ETL.
- Airflow используется для оркестрации и автоматизации процесса.
- PostgreSQL является основной базой данных для хранения сырых и обработанных данных.

### Замечания

- Для сайта ТАСС и Ведомости, где не предоставляется текст новости в RSS-ленте, используется веб-скрапинг с использованием библиотеки BeautifulSoup для получения текста новости.
- Проект предусматривает обработку дубликатов по ссылкам на новость, а также унификацию категорий для обеспечения единообразия данных.

## Описание DAG

1. **get_category_groupe:**
   - Задача для извлечения и загрузки категорий и подкатегорий в базу данных.

2. **get_site_name:**
   - Задача для извлечения и загрузки уникальных имен сайтов и их источников в базу данных.

3. **get_authors:**
   - Задача для извлечения и загрузки уникальных авторов новостей в базу данных.

4. **get_category:**
   - Задача для извлечения и загрузки уникальных категорий новостей в базу данных.

5. **extract_and_fill_news:**
   - Задача для извлечения и загрузки новостей в базу данных.

6. **fill_data_news:**
   - Задача для извлечения и загрузки связанных данных (категорий, сайтов, авторов) для новостей в базу данных.

7. **execute_sql**:
   - Задача, использующая PostgresOperator, для выполнения SQL-запроса в DAG.

## Замечания к SQL-запросу

- В SQL-запросе проводится анализ данных, включая подсчет общего количества новостей, количества новостей за последний день, среднего количества публикаций в день и других метрик.
- Используется оконная функция RANK() для определения дня с максимальным количеством публикаций для каждой категории.
- SQL-запрос выдаст информацию о категориях, сайтах, днях максимального количества публикаций, общем количестве новостей и других параметрах для анализа.

Проект обеспечивает удобный механизм для анализа и мониторинга публикаций новостей из различных источников, а также позволяет выявлять тенденции и популярные темы в определенные дни.
